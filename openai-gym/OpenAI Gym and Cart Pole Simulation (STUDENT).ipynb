{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Gym and Cart Pole Simulation (STUDENT)\n",
    "Billy Hau\n",
    "8/16/2022\n",
    "\n",
    "<img src='https://www.gymlibrary.ml/_images/cart_pole.gif'>\n",
    "\n",
    "Cart Pole is another classical problem that is being discussed in Control System class. It is basically like trying to balance a pencil on your finger tip. How do you move your hand to balance the pencil? Except here, we have a cart, and the pencil is represented by a pole.\n",
    "\n",
    "<a href='https://www.gymlibrary.ml/environments/classic_control/cart_pole/'>https://www.gymlibrary.ml/environments/classic_control/cart_pole/</a>\n",
    "\n",
    "<b>Please note that everytime you close the simulation window or create a new environment, you will need to restart the notebook!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Environment\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Simulation Display as Image Array\n",
    "\n",
    "This is how to retrieve the game screen as image... not useful right now, since we can extract different game data. But in real life, you would do that to train on the game image using CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libGL error: MESA-LOADER: failed to open iris: /opt/miniconda3/envs/gym/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /usr/lib/dri/iris_dri.so) (search paths /usr/lib/dri, suffix _dri)\n",
      "libGL error: failed to load driver: iris\n",
      "libGL error: MESA-LOADER: failed to open iris: /opt/miniconda3/envs/gym/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /usr/lib/dri/iris_dri.so) (search paths /usr/lib/dri, suffix _dri)\n",
      "libGL error: failed to load driver: iris\n",
      "libGL error: MESA-LOADER: failed to open swrast: /opt/miniconda3/envs/gym/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /usr/lib/dri/swrast_dri.so) (search paths /usr/lib/dri, suffix _dri)\n",
      "libGL error: failed to load driver: swrast\n"
     ]
    },
    {
     "ename": "ContextException",
     "evalue": "Could not create GL context",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mContextException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/wcyat/dev/kaggle/code/openai-gym/OpenAI Gym and Cart Pole Simulation (STUDENT).ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wcyat/dev/kaggle/code/openai-gym/OpenAI%20Gym%20and%20Cart%20Pole%20Simulation%20%28STUDENT%29.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m env\u001b[39m.\u001b[39mreset() \u001b[39m# Need to call reset before the start of simulation\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/wcyat/dev/kaggle/code/openai-gym/OpenAI%20Gym%20and%20Cart%20Pole%20Simulation%20%28STUDENT%29.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m env\u001b[39m.\u001b[39;49mrender(mode \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mrgb_array\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gym/lib/python3.8/site-packages/gym/core.py:295\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(mode, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gym/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py:179\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    176\u001b[0m cartheight \u001b[39m=\u001b[39m \u001b[39m30.0\u001b[39m\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviewer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassic_control\u001b[39;00m \u001b[39mimport\u001b[39;00m rendering\n\u001b[1;32m    181\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviewer \u001b[39m=\u001b[39m rendering\u001b[39m.\u001b[39mViewer(screen_width, screen_height)\n\u001b[1;32m    182\u001b[0m     l, r, t, b \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mcartwidth \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, cartwidth \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, cartheight \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, \u001b[39m-\u001b[39mcartheight \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gym/lib/python3.8/site-packages/gym/envs/classic_control/rendering.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     18\u001b[0m         \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m    Cannot import pyglet.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     26\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpyglet\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgl\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     29\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     30\u001b[0m         \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m    Error occurred while running `from pyglet.gl import *`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gym/lib/python3.8/site-packages/pyglet/gl/__init__.py:232\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_pyglet_doc_run \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mpyglet.window\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _sys\u001b[39m.\u001b[39mmodules \u001b[39mand\u001b[39;00m _pyglet\u001b[39m.\u001b[39moptions[\u001b[39m'\u001b[39m\u001b[39mshadow_window\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m    230\u001b[0m     \u001b[39m# trickery is for circular import\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     _pyglet\u001b[39m.\u001b[39mgl \u001b[39m=\u001b[39m _sys\u001b[39m.\u001b[39mmodules[\u001b[39m__name__\u001b[39m]\n\u001b[0;32m--> 232\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpyglet\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwindow\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gym/lib/python3.8/site-packages/pyglet/window/__init__.py:1918\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_pyglet_doc_run:\n\u001b[1;32m   1917\u001b[0m     pyglet\u001b[39m.\u001b[39mwindow \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mmodules[\u001b[39m__name__\u001b[39m]\n\u001b[0;32m-> 1918\u001b[0m     gl\u001b[39m.\u001b[39;49m_create_shadow_window()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gym/lib/python3.8/site-packages/pyglet/gl/__init__.py:206\u001b[0m, in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyglet\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwindow\u001b[39;00m \u001b[39mimport\u001b[39;00m Window\n\u001b[0;32m--> 206\u001b[0m _shadow_window \u001b[39m=\u001b[39m Window(width\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, height\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, visible\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    207\u001b[0m _shadow_window\u001b[39m.\u001b[39mswitch_to()\n\u001b[1;32m    209\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyglet\u001b[39;00m \u001b[39mimport\u001b[39;00m app\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gym/lib/python3.8/site-packages/pyglet/window/xlib/__init__.py:171\u001b[0m, in \u001b[0;36mXlibWindow.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_handlers[message] \u001b[39m=\u001b[39m func\n\u001b[0;32m--> 171\u001b[0m \u001b[39msuper\u001b[39;49m(XlibWindow, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    173\u001b[0m \u001b[39mglobal\u001b[39;00m _can_detect_autorepeat\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m _can_detect_autorepeat \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gym/lib/python3.8/site-packages/pyglet/window/__init__.py:615\u001b[0m, in \u001b[0;36mBaseWindow.__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, file_drops, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    612\u001b[0m     config \u001b[39m=\u001b[39m screen\u001b[39m.\u001b[39mget_best_config(config)\n\u001b[1;32m    614\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m context:\n\u001b[0;32m--> 615\u001b[0m     context \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39;49mcreate_context(gl\u001b[39m.\u001b[39;49mcurrent_context)\n\u001b[1;32m    617\u001b[0m \u001b[39m# Set these in reverse order to above, to ensure we get user preference\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_context \u001b[39m=\u001b[39m context\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gym/lib/python3.8/site-packages/pyglet/gl/xlib.py:204\u001b[0m, in \u001b[0;36mXlibCanvasConfig13.create_context\u001b[0;34m(self, share)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_context\u001b[39m(\u001b[39mself\u001b[39m, share):\n\u001b[1;32m    203\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglx_info\u001b[39m.\u001b[39mhave_extension(\u001b[39m'\u001b[39m\u001b[39mGLX_ARB_create_context\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 204\u001b[0m         \u001b[39mreturn\u001b[39;00m XlibContextARB(\u001b[39mself\u001b[39;49m, share)\n\u001b[1;32m    205\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m         \u001b[39mreturn\u001b[39;00m XlibContext13(\u001b[39mself\u001b[39m, share)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gym/lib/python3.8/site-packages/pyglet/gl/xlib.py:314\u001b[0m, in \u001b[0;36mXlibContext13.__init__\u001b[0;34m(self, config, share)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config, share):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39msuper\u001b[39;49m(XlibContext13, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(config, share)\n\u001b[1;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglx_window \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gym/lib/python3.8/site-packages/pyglet/gl/xlib.py:218\u001b[0m, in \u001b[0;36mBaseXlibContext.__init__\u001b[0;34m(self, config, share)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglx_context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_glx_context(share)\n\u001b[1;32m    216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglx_context:\n\u001b[1;32m    217\u001b[0m     \u001b[39m# TODO: Check Xlib error generated\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mraise\u001b[39;00m gl\u001b[39m.\u001b[39mContextException(\u001b[39m'\u001b[39m\u001b[39mCould not create GL context\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    220\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_have_SGI_video_sync \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mglx_info\u001b[39m.\u001b[39mhave_extension(\u001b[39m'\u001b[39m\u001b[39mGLX_SGI_video_sync\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    221\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_have_SGI_swap_control \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mglx_info\u001b[39m.\u001b[39mhave_extension(\u001b[39m'\u001b[39m\u001b[39mGLX_SGI_swap_control\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mContextException\u001b[0m: Could not create GL context"
     ]
    }
   ],
   "source": [
    "env.reset() # Need to call reset before the start of simulation\n",
    "env.render(mode = 'rgb_array') # render_mode = 'rgb_array' for getting image array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset() # Image is cleared once retrieved, so need to reset or step\n",
    "img = env.render(mode = 'rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 600, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the image array dimension\n",
    "img = np.array(img)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f47cce01ba0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot after reshaping\n",
    "plt.imshow(img.reshape(400,600,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Gym Human Display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.gymlibrary.ml/_images/AE_loop_dark.png' width='300px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation Space\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Num</th>\n",
    "        <th>Observation</th>\n",
    "        <th>Min</th>\n",
    "        <th>Max</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>Cart Position</td>\n",
    "        <td>-4.8</td>\n",
    "        <td>4.8</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>Cart Velocity</td>\n",
    "        <td>-Inf</td>\n",
    "        <td>Inf</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>Pole Angle</td>\n",
    "        <td>~ -0.418 rad (-24°)</td>\n",
    "        <td>~ 0.418 rad (24°)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>Pole Angular Velocity</td>\n",
    "        <td>-Inf</td>\n",
    "        <td>Inf</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00371341, -0.03631802, -0.02009435, -0.02392558], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset and Retrieve the Initial Observation\n",
    "observation = env.reset()\n",
    "observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action Space\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Num</th>\n",
    "        <th>Action</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>Push Cart to the Left</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>Push Cart to the Right</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action Space\n",
    "# 0 - Accelerate to the Left\n",
    "# 1 - No Acceleration\n",
    "# 2 - Accelerate to the Right\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's Try a Single Step to the Right and Render\n",
    "action = 1\n",
    "env.step(action)\n",
    "env.render(mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's T Keep Moving Right for 50 Steps\n",
    "observation = env.reset()\n",
    "\n",
    "for i in range(50):\n",
    "    action = 1\n",
    "    env.step(action)\n",
    "    env.render(mode='human') \n",
    "    time.sleep(0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 1.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [-0.04313922 -0.00732914  0.04026072 -0.00819511]\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "# Let's Try to Collect Some Data\n",
    "observation = env.reset()\n",
    "\n",
    "for i in range(50):\n",
    "    action = 1\n",
    "    __________, reward, done, info = env.step(action) \n",
    "    env.render(mode='human') \n",
    "\n",
    "    print(f'observation - {observation}')\n",
    "    print(f'reward - {reward}')\n",
    "    print(f'done - {done}')\n",
    "    print('--------------')\n",
    "    time.sleep(0.02)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control System (Random Action)\n",
    "\n",
    "What happens if we apply random action here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try making some random moves and see what it will do...\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation - [-0.0074904   0.21138191  0.04373346 -0.27681008]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [-0.00326276  0.0156642   0.03819726  0.02933932]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [-0.00294948 -0.17998411  0.03878405  0.333825  ]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [-0.00654916 -0.37563598  0.04546055  0.63848174]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [-0.01406188 -0.18117642  0.05823018  0.36045456]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [-0.01768541  0.01307155  0.06543928  0.08668645]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [-0.01742398  0.20719741  0.067173   -0.18465368]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [-0.01328003  0.40129715  0.06347993 -0.45541266]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [-0.00525409  0.5954668   0.05437168 -0.7274299 ]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.00665525  0.399637    0.03982308 -0.4181419 ]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.01464799  0.20397402  0.03146024 -0.11317523]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.01872747  0.3986314   0.02919674 -0.395769  ]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.0267001   0.2031076   0.02128135 -0.09402571]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.03076225 0.00768719 0.01940084 0.20529477]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.03091599 -0.18770675  0.02350674  0.50403404]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.02716186 0.00707616 0.03358741 0.21885072]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.02730338  0.20170228  0.03796443 -0.06305098]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.03133743 0.00605716 0.03670341 0.2413641 ]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.03145857  0.20063612  0.04153069 -0.03951938]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.03547129  0.39513868  0.04074031 -0.3188151 ]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.04337407  0.5896574   0.034364   -0.5983768 ]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.05516721  0.78428215  0.02239647 -0.88004047]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.07085285  0.5888632   0.00479566 -0.58040154]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.08263012  0.39367434 -0.00681237 -0.28621176]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.09050361  0.19865021 -0.01253661  0.00431483]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.09447661  0.3939497  -0.01245031 -0.29229704]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.10235561  0.5892469  -0.01829625 -0.5888805 ]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.11414055  0.3943859  -0.03007386 -0.30201665]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.12202826  0.58992326 -0.0361142  -0.6040305 ]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.13382673  0.39532453 -0.04819481 -0.32293794]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.14173321  0.2009208  -0.05465356 -0.04583474]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.14575164  0.00662341 -0.05557026  0.22911602]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.1458841  -0.18766221 -0.05098794  0.50376534]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.14213085  0.00813988 -0.04091263  0.19545995]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.14229366  0.20382245 -0.03700343 -0.10984323]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.1463701   0.39945456 -0.03920029 -0.41396707]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.15435919  0.5951096  -0.04747964 -0.71874607]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.16626139  0.40067568 -0.06185456 -0.44137764]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.1742749   0.5966159  -0.07068212 -0.7528996 ]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.18620722  0.40253606 -0.0857401  -0.48326993]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.19425794  0.59875685 -0.0954055  -0.8016966 ]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.20623308  0.79504865 -0.11143944 -1.122802  ]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.22213405  0.99144113 -0.13389547 -1.4482589 ]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.24196288  1.1879311  -0.16286065 -1.7796035 ]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.2657215   0.9949739  -0.19845273 -1.5416673 ]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.285621    1.1918508  -0.22928607 -1.8891492 ]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 0.309458    1.3886119  -0.26706904 -2.2433877 ]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 0.33723024  1.5850874  -0.3119368  -2.6052413 ]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 0.36893198  1.7810159  -0.36404163 -2.9751801 ]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 0.40455228  1.5893685  -0.42354524 -2.8112285 ]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 0.43633965  1.7842859  -0.4797698  -3.198602  ]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 0.47202536  1.9776584  -0.5437418  -3.5916166 ]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 0.51157856  1.786126   -0.6155742  -3.497851  ]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 0.54730105  1.5955228  -0.6855312  -3.4341898 ]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 0.57921153  1.7848366  -0.754215   -3.8401337 ]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 0.6149083  1.5935816 -0.8310177 -3.8323576]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 0.6467799   1.7779031  -0.90766484 -4.2358947 ]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 0.68233794  1.9580029  -0.9923827  -4.633888  ]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 0.721498   2.1331625 -1.0850605 -5.0237017]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 0.7641612  2.3027284 -1.1855345 -5.40244  ]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 0.81021583  2.0990143  -1.2935833  -5.5600557 ]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 0.8521961  2.258132  -1.4047844 -5.9081516]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 0.8973588  2.0468016 -1.5229474 -6.1457257]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 0.93829477  1.8312912  -1.645862   -6.4239273 ]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 0.97492063  1.6109796  -1.7743405  -6.741883  ]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.0071402  1.7500718 -1.9091781 -6.987639 ]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.0421417  1.8868619 -2.048931  -7.1968536]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.0798788  2.0233858 -2.192868  -7.363656 ]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.1203465  1.7897706 -2.340141  -7.8067803]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.156142   1.5536827 -2.4962769 -8.264338 ]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.1872157  1.6980163 -2.6615634 -8.2682   ]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.221176   1.469781  -2.8269274 -8.707631 ]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.2505716  1.2490906 -3.00108   -9.113405 ]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.2755533  1.4306152 -3.1833482 -8.884976 ]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.3041657  1.6295248 -3.3610477 -8.5746   ]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.3367562  1.4536755 -3.5325398 -8.768044 ]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.3658297  1.6800263 -3.7079005 -8.342103 ]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.3994303  1.5309634 -3.8747427 -8.3730545]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.4300495  1.3932585 -4.042204  -8.329792 ]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.4579147  1.2639792 -4.2088    -8.219831 ]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.4831942  1.1396329 -4.373196  -8.05234  ]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.5059869  1.0166668 -4.534243  -7.836461 ]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.5263202  1.2562724 -4.6909723 -7.483425 ]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.5514457  1.1256353 -4.8406405 -7.193689 ]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.5739584  1.3526698 -4.9845147 -6.94566  ]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.6010118  1.2089266 -5.123428  -6.604526 ]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.6251904  1.4245484 -5.2555184 -6.464246 ]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.6536813  1.6368412 -5.3848033 -6.3771286]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.6864182  1.8466195 -5.512346  -6.3431253]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.7233505  2.0545382 -5.6392083 -6.362003 ]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.7644413  1.8808199 -5.7664485 -5.977103 ]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.8020576  2.0833836 -5.8859906 -6.0960283]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.8437253  2.2852046 -6.0079107 -6.261463 ]\n",
      "action - 1\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.8894295  2.0977414 -6.13314   -5.9109426]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.9313843  1.9059087 -6.251359  -5.5824785]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 1.9695024  1.7113125 -6.3630085 -5.281377 ]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 2.0037286  1.5152519 -6.468636  -5.0116653]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 2.0340338  1.3186997 -6.5688696 -4.776103 ]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 2.0604076  1.122325  -6.6643915 -4.576333 ]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n",
      "observation - [ 2.0828543  0.926537  -6.755918  -4.4131126]\n",
      "action - 0\n",
      "reward - 0.0\n",
      "done - True\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "# Random Action Controls\n",
    "observation = env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action) \n",
    "    env.render(mode='human') \n",
    "\n",
    "    print(f'observation - {observation}')\n",
    "    print(f'action - {action}')\n",
    "    print(f'reward - {reward}')\n",
    "    print(f'done - {done}')\n",
    "    print('--------------')\n",
    "    time.sleep(0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control System (Can you solve the simulation?)\n",
    "\n",
    "Think about how you would solve this problem manually..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Policy (Control System) that will help us reach the goal!\n",
    "def policy(observation):\n",
    "    pole_angle = observation[2]\n",
    "    pole_angle_vel = observation[3]\n",
    "\n",
    "    if pole_angle < pole_angle_vel:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation - [ 0.02333003 -0.19604062  0.0117072   0.25374225]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.01940922 -0.00108776  0.01678205 -0.03522514]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.01938746 -0.1964463   0.01607754  0.2627051 ]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.01545853 -0.00155749  0.02133165 -0.02486375]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.01542738 -0.19697876  0.02083437  0.27447248]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.01148781 -0.00216017  0.02632382 -0.01156711]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.0114446  -0.19764955  0.02609248  0.2893037 ]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.00749161 -0.00290921  0.03187855  0.00496292]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.00743343 -0.1984735   0.03197781  0.30753103]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.00346396 -0.00382145  0.03812843  0.02510209]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.00338753 -0.19946887  0.03863047  0.32956696]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [-0.00060185 -0.00491752  0.04522181  0.0493122 ]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [-0.0007002   0.1895278   0.04620806 -0.22876692]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.00309036 -0.00622298  0.04163272  0.07812617]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.0029659   0.18827818  0.04319524 -0.20113628]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.00673146 -0.00743408  0.03917252  0.10485373]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.00658278  0.18710525  0.04126959 -0.17521763]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.01032489 -0.00858229  0.03776524  0.13019356]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.01015324  0.1859789   0.04036911 -0.15033978]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.01387282 -0.00969717  0.03736231  0.15480047]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.01367887  0.18487047  0.04045832 -0.12586533]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.01737628 -0.01080704  0.03794102  0.17930223]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.01716014  0.18375202  0.04152706 -0.10117432]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.02083518 -0.01193972  0.03950357  0.20431565]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.02059639  0.18259569  0.04358989 -0.07564878]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.0242483  -0.01312318  0.04207691  0.23046203]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.02398584  0.18137304  0.04668615 -0.04865711]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.0276133  -0.01438618  0.04571301  0.2583825 ]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.02732558  0.18005435  0.05088066 -0.01953867]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.03092666 -0.01575896  0.05048989  0.28875366]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.03061148 0.178608   0.05626496 0.01241247]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.03418364 -0.01727383  0.05651321  0.32230344]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.03383817 0.17699973 0.06295928 0.04796435]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.03737816 -0.01896586  0.06391857  0.35982814]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.03699885 0.17519201 0.07111513 0.08796534]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.04050269  0.36922637  0.07287443 -0.18146029]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.04788721 0.17314138 0.06924523 0.13329265]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.05135004  0.36720663  0.07191108 -0.13676581]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.05869417 0.1711323  0.06917576 0.1777092 ]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.06211682  0.36519963  0.07272995 -0.09237408]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.06942081 0.16911465 0.07088247 0.22233956]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.0728031   0.36315566  0.07532926 -0.04716905]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.08006622 0.16703889 0.07438588 0.2682977 ]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 8.3406992e-02  3.6102480e-01  7.9751834e-02 -2.7207709e-05]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.09062749 0.16485503 0.07975129 0.31671435]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.09392459 0.35875583 0.08608557 0.05020964]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.10109971 0.16251174 0.08708977 0.36876363]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.10434994 0.35629526 0.09446504 0.10476201]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.11147585  0.5499454   0.09656028 -0.15668672]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.12247475 0.35358313 0.09342655 0.1648293 ]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.12954642  0.5472521   0.09672313 -0.09697983]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.14049146 0.35088652 0.09478354 0.22458409]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.14750919  0.54453504  0.09927522 -0.03676017]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.1583999  0.34813994 0.09854002 0.28552094]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.16536269 0.54172856 0.10425043 0.02547063]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.17619726 0.34527802 0.10475984 0.3491407 ]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.18310282 0.53876615 0.11174266 0.09124273]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.19387814 0.34223467 0.11356752 0.4169854 ]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.20072284 0.53557944 0.12190722 0.16215368]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.21143442  0.7287644   0.1251503  -0.0897212 ]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.22600971 0.5320915  0.12335587 0.23967907]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.23665154  0.72525513  0.12814945 -0.01169   ]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.25115666 0.5285505  0.12791565 0.31852046]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.26172766 0.7216406  0.13428606 0.06875635]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.27616048 0.52487457 0.13566118 0.40060982]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.28665796 0.71783763 0.14367339 0.15358675]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.30101472  0.91064143  0.14674512 -0.09054437]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.31922755 0.7137542  0.14493424 0.24460073]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.33350262 0.90654075 0.14982624 0.00091448]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.35163343 0.7096231  0.14984454 0.33686614]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.3658259  0.90233034 0.15658186 0.09493398]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.3838725  0.7053512  0.15848054 0.4326352 ]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.39797953 0.8979161  0.16713324 0.19380614]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [ 0.41593784  1.090302    0.17100936 -0.04184288]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.4377439  0.893193   0.17017251 0.29953995]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.45560774 1.0855324  0.1761633  0.0649877 ]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.4773184  0.8883798  0.17746305 0.40766433]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.49508598 1.0806001  0.18561634 0.17576294]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.516698   0.8833739  0.1891316  0.52077883]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.5343655  1.0754008  0.19954719 0.2931487 ]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.5558735  1.2672014  0.20541015 0.06943499]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.5812175  1.0698177  0.20679885 0.41925326]\n",
      "action - 0\n",
      "reward - 1.0\n",
      "done - False\n",
      "--------------\n",
      "observation - [0.60261387 1.2615016  0.21518393 0.19822013]\n",
      "action - 1\n",
      "reward - 1.0\n",
      "done - True\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "# Let's Run the Simulation!\n",
    "observation = env.reset()\n",
    "\n",
    "for i in range(300):\n",
    "    action = policy(observation)\n",
    "    observation, reward, done, info = env.step(action) \n",
    "    env.render(mode='human') \n",
    "\n",
    "    print(f'observation - {observation}')\n",
    "    print(f'action - {action}')\n",
    "    print(f'reward - {reward}')\n",
    "    print(f'done - {done}')\n",
    "    print('--------------')\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "    time.sleep(0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning with Stable-Baseline3\n",
    "\n",
    "Let's say we are lazy and can't figure out a good logic for controling. Can we use AI to figure it out? Apparently, we can! Here, we will use Reinforcement Learning to train an AI to solve this problem through a process called self-play. Basically, the AI will play the simulation a bunch of times and learn from it's mistakes through rewards and punishment. \n",
    "\n",
    "There are many Reinforcement Learning algorithms, we will use Proximal Policy Optmization (PPO) here, since it is what OpenAI used to train it's super smart AI that can beat human!\n",
    "\n",
    "We will use the Stable-Baselines3 library. It basically hide away all the complexity with tuning the rewards and connecting the environment... but we will learn more later in Unity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define PPO Reinforcement Learning Model (MLP Policy - ANN)\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/wcyat/dev/kaggle/code/openai-gym/OpenAI Gym and Cart Pole Simulation (STUDENT).ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wcyat/dev/kaggle/code/openai-gym/OpenAI%20Gym%20and%20Cart%20Pole%20Simulation%20%28STUDENT%29.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m observation \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wcyat/dev/kaggle/code/openai-gym/OpenAI%20Gym%20and%20Cart%20Pole%20Simulation%20%28STUDENT%29.ipynb#X36sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m500\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/wcyat/dev/kaggle/code/openai-gym/OpenAI%20Gym%20and%20Cart%20Pole%20Simulation%20%28STUDENT%29.ipynb#X36sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     action, _state \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(observation, deterministic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wcyat/dev/kaggle/code/openai-gym/OpenAI%20Gym%20and%20Cart%20Pole%20Simulation%20%28STUDENT%29.ipynb#X36sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wcyat/dev/kaggle/code/openai-gym/OpenAI%20Gym%20and%20Cart%20Pole%20Simulation%20%28STUDENT%29.ipynb#X36sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     env\u001b[39m.\u001b[39mrender()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize Blank Model \n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "for i in range(500):\n",
    "    action, _state = model.predict(observation, deterministic=True)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    print(f'Reward: {reward}')\n",
    "\n",
    "    time.sleep(0.02)\n",
    "\n",
    "    if done:\n",
    "        print('Reset')\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.2     |\n",
      "|    ep_rew_mean     | 23.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 1644     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.5        |\n",
      "|    ep_rew_mean          | 29.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1174        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009007321 |\n",
      "|    clip_fraction        | 0.0935      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.000604    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.06        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    value_loss           | 52.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 34.6        |\n",
      "|    ep_rew_mean          | 34.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1046        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007382406 |\n",
      "|    clip_fraction        | 0.0439      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.0939      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.1        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 40.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 44          |\n",
      "|    ep_rew_mean          | 44          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1002        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011957348 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.626      |\n",
      "|    explained_variance   | 0.285       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.8        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 49.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 57.3        |\n",
      "|    ep_rew_mean          | 57.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 968         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009556277 |\n",
      "|    clip_fraction        | 0.0987      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.598      |\n",
      "|    explained_variance   | 0.255       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.9        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    value_loss           | 65.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 70.9        |\n",
      "|    ep_rew_mean          | 70.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 951         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010592394 |\n",
      "|    clip_fraction        | 0.0578      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.59       |\n",
      "|    explained_variance   | 0.378       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.4        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 56.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 87.3        |\n",
      "|    ep_rew_mean          | 87.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 905         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005848958 |\n",
      "|    clip_fraction        | 0.066       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.587      |\n",
      "|    explained_variance   | 0.633       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.37        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 39.6        |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 104       |\n",
      "|    ep_rew_mean          | 104       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 892       |\n",
      "|    iterations           | 8         |\n",
      "|    time_elapsed         | 18        |\n",
      "|    total_timesteps      | 16384     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0049841 |\n",
      "|    clip_fraction        | 0.0453    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.571    |\n",
      "|    explained_variance   | 0.681     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 11        |\n",
      "|    n_updates            | 70        |\n",
      "|    policy_gradient_loss | -0.00775  |\n",
      "|    value_loss           | 47.9      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f1d55c8cfa0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Model with the Environment\n",
    "# Stable-Baselines is so easy that it handle all the rewards and stuffs internally\n",
    "# If it doesn't do well, just keep training... like a lot! Like 1 million cycle? :)\n",
    "# If it doesn't converge, maybe try recreate the model to reinitialize the weights\n",
    "# yeah, it's a lot of trial and error...\n",
    "\n",
    "model.learn(total_timesteps=15000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ContextException",
     "evalue": "Could not create GL context",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mContextException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/wcyat/dev/kaggle/code/openai-gym/OpenAI Gym and Cart Pole Simulation (STUDENT).ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wcyat/dev/kaggle/code/openai-gym/OpenAI%20Gym%20and%20Cart%20Pole%20Simulation%20%28STUDENT%29.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m action, _state \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(observation, deterministic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wcyat/dev/kaggle/code/openai-gym/OpenAI%20Gym%20and%20Cart%20Pole%20Simulation%20%28STUDENT%29.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/wcyat/dev/kaggle/code/openai-gym/OpenAI%20Gym%20and%20Cart%20Pole%20Simulation%20%28STUDENT%29.ipynb#X41sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m env\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wcyat/dev/kaggle/code/openai-gym/OpenAI%20Gym%20and%20Cart%20Pole%20Simulation%20%28STUDENT%29.ipynb#X41sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mReward: \u001b[39m\u001b[39m{\u001b[39;00mreward\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/wcyat/dev/kaggle/code/openai-gym/OpenAI%20Gym%20and%20Cart%20Pole%20Simulation%20%28STUDENT%29.ipynb#X41sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m0.02\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gym/lib/python3.8/site-packages/gym/core.py:295\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(mode, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gym/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py:179\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    176\u001b[0m cartheight \u001b[39m=\u001b[39m \u001b[39m30.0\u001b[39m\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviewer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassic_control\u001b[39;00m \u001b[39mimport\u001b[39;00m rendering\n\u001b[1;32m    181\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviewer \u001b[39m=\u001b[39m rendering\u001b[39m.\u001b[39mViewer(screen_width, screen_height)\n\u001b[1;32m    182\u001b[0m     l, r, t, b \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mcartwidth \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, cartwidth \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, cartheight \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, \u001b[39m-\u001b[39mcartheight \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gym/lib/python3.8/site-packages/gym/envs/classic_control/rendering.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     18\u001b[0m         \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m    Cannot import pyglet.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     26\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpyglet\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgl\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     29\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     30\u001b[0m         \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m    Error occurred while running `from pyglet.gl import *`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gym/lib/python3.8/site-packages/pyglet/gl/__init__.py:232\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_pyglet_doc_run \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mpyglet.window\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _sys\u001b[39m.\u001b[39mmodules \u001b[39mand\u001b[39;00m _pyglet\u001b[39m.\u001b[39moptions[\u001b[39m'\u001b[39m\u001b[39mshadow_window\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m    230\u001b[0m     \u001b[39m# trickery is for circular import\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     _pyglet\u001b[39m.\u001b[39mgl \u001b[39m=\u001b[39m _sys\u001b[39m.\u001b[39mmodules[\u001b[39m__name__\u001b[39m]\n\u001b[0;32m--> 232\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpyglet\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwindow\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gym/lib/python3.8/site-packages/pyglet/window/__init__.py:1918\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_pyglet_doc_run:\n\u001b[1;32m   1917\u001b[0m     pyglet\u001b[39m.\u001b[39mwindow \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mmodules[\u001b[39m__name__\u001b[39m]\n\u001b[0;32m-> 1918\u001b[0m     gl\u001b[39m.\u001b[39;49m_create_shadow_window()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gym/lib/python3.8/site-packages/pyglet/gl/__init__.py:206\u001b[0m, in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyglet\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwindow\u001b[39;00m \u001b[39mimport\u001b[39;00m Window\n\u001b[0;32m--> 206\u001b[0m _shadow_window \u001b[39m=\u001b[39m Window(width\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, height\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, visible\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    207\u001b[0m _shadow_window\u001b[39m.\u001b[39mswitch_to()\n\u001b[1;32m    209\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyglet\u001b[39;00m \u001b[39mimport\u001b[39;00m app\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gym/lib/python3.8/site-packages/pyglet/window/xlib/__init__.py:171\u001b[0m, in \u001b[0;36mXlibWindow.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_handlers[message] \u001b[39m=\u001b[39m func\n\u001b[0;32m--> 171\u001b[0m \u001b[39msuper\u001b[39;49m(XlibWindow, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    173\u001b[0m \u001b[39mglobal\u001b[39;00m _can_detect_autorepeat\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m _can_detect_autorepeat \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gym/lib/python3.8/site-packages/pyglet/window/__init__.py:615\u001b[0m, in \u001b[0;36mBaseWindow.__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, file_drops, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    612\u001b[0m     config \u001b[39m=\u001b[39m screen\u001b[39m.\u001b[39mget_best_config(config)\n\u001b[1;32m    614\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m context:\n\u001b[0;32m--> 615\u001b[0m     context \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39;49mcreate_context(gl\u001b[39m.\u001b[39;49mcurrent_context)\n\u001b[1;32m    617\u001b[0m \u001b[39m# Set these in reverse order to above, to ensure we get user preference\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_context \u001b[39m=\u001b[39m context\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gym/lib/python3.8/site-packages/pyglet/gl/xlib.py:204\u001b[0m, in \u001b[0;36mXlibCanvasConfig13.create_context\u001b[0;34m(self, share)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_context\u001b[39m(\u001b[39mself\u001b[39m, share):\n\u001b[1;32m    203\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglx_info\u001b[39m.\u001b[39mhave_extension(\u001b[39m'\u001b[39m\u001b[39mGLX_ARB_create_context\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 204\u001b[0m         \u001b[39mreturn\u001b[39;00m XlibContextARB(\u001b[39mself\u001b[39;49m, share)\n\u001b[1;32m    205\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m         \u001b[39mreturn\u001b[39;00m XlibContext13(\u001b[39mself\u001b[39m, share)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gym/lib/python3.8/site-packages/pyglet/gl/xlib.py:314\u001b[0m, in \u001b[0;36mXlibContext13.__init__\u001b[0;34m(self, config, share)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config, share):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39msuper\u001b[39;49m(XlibContext13, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(config, share)\n\u001b[1;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglx_window \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/gym/lib/python3.8/site-packages/pyglet/gl/xlib.py:218\u001b[0m, in \u001b[0;36mBaseXlibContext.__init__\u001b[0;34m(self, config, share)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglx_context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_glx_context(share)\n\u001b[1;32m    216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglx_context:\n\u001b[1;32m    217\u001b[0m     \u001b[39m# TODO: Check Xlib error generated\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mraise\u001b[39;00m gl\u001b[39m.\u001b[39mContextException(\u001b[39m'\u001b[39m\u001b[39mCould not create GL context\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    220\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_have_SGI_video_sync \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mglx_info\u001b[39m.\u001b[39mhave_extension(\u001b[39m'\u001b[39m\u001b[39mGLX_SGI_video_sync\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    221\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_have_SGI_swap_control \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mglx_info\u001b[39m.\u001b[39mhave_extension(\u001b[39m'\u001b[39m\u001b[39mGLX_SGI_swap_control\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mContextException\u001b[0m: Could not create GL context"
     ]
    }
   ],
   "source": [
    "# Visualize Trained Model \n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "    action, _state = model.predict(observation, deterministic=True)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    print(f'Reward: {reward}')\n",
    "\n",
    "    time.sleep(0.02)\n",
    "\n",
    "    if done:\n",
    "        print('Reset')\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discover_other_daemon: 1Requirement already satisfied: pyglet in /opt/miniconda3/envs/gym/lib/python3.8/site-packages (1.5.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyglet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy, right? We quicked tacked a difficult control system problem via Reinforcement Learning! Now, can you do the same with the Mountain Car problem? You might need to restart the kernel, close the simulation windows and just run the following:\n",
    "\n",
    "<img src='https://www.gymlibrary.ml/_images/mountain_car.gif'>\n",
    "\n",
    "<a href='https://www.gymlibrary.ml/environments/classic_control/mountain_car/'>https://www.gymlibrary.ml/environments/classic_control/mountain_car/</href>\n",
    "\n",
    "This is a simulation that take into account of physics and momentum. You will need to build up enough speed to get to the top of the mountain. Can you use RL to achieve this? Look at how to implement PPO model above...\n",
    "\n",
    "#### Observation Space\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Num</th>\n",
    "        <th>Observation</th>\n",
    "        <th>Min</th>\n",
    "        <th>Max</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>Position of the Car Alone X-Axis</td>\n",
    "        <td>-Inf</td>\n",
    "        <td>Inf</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>Velocity of the Car</td>\n",
    "        <td>-Inf</td>\n",
    "        <td>Inf</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Action Space\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Num</th>\n",
    "        <th>Action</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>Accelerate to the Left</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>None</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>Accelerate to the Right</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Environment\n",
    "env = gym.make('MountainCar-v0') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Run the Simulation!\n",
    "observation = env.reset()\n",
    "\n",
    "for i in range(300):\n",
    "    action = 2\n",
    "    observation, reward, done, info = env.step(action) \n",
    "    env.render(mode='human') \n",
    "\n",
    "    print(f'observation - {observation}')\n",
    "    print(f'action - {action}')\n",
    "    print(f'reward - {reward}')\n",
    "    print(f'done - {done}')\n",
    "    print('--------------')\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "    time.sleep(0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ddd107cbd7c012cd196d94b824e2e6196ee685cadf0541192cb15d2c9f67d769"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
